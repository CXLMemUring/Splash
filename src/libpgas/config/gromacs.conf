# PGAS Configuration for GROMACS Molecular Dynamics
# ==================================================
# GROMACS performs molecular dynamics simulations
# Characteristics:
#   - Neighbor-list based memory access patterns
#   - Non-bonded force calculations dominate compute
#   - SIMD-vectorized kernels for efficiency
#   - NUMA-aware with domain decomposition
#   - Frequent CPU-GPU memory transfers when GPU-accelerated
#   - PME (Particle Mesh Ewald) for long-range electrostatics
#
# Tuning Strategy:
#   - Exploit neighbor-list predictability for prefetching
#   - Domain decomposition aligns with PGAS partitioning
#   - NUMA-aware allocation for multi-socket systems
#   - Optimize for periodic boundary conditions

# Node configuration
local_node_id=0
num_nodes=2

# Node addresses (IP:Port:CXL_Base:CXL_Size)
# GROMACS memory depends on system size
# Typical: 1-10GB for million-atom systems
node0=127.0.0.1:5000:0x0:8589934592
node1=127.0.0.1:5001:0x0:8589934592

# GROMACS-specific tuning parameters
# ===================================

# Memory affinity: LOCAL with domain decomposition
# Each MPI rank owns a spatial domain - keep domain data local
# Cross-domain communication handled via halo exchange
memory_affinity=local

# Partitioning: BLOCK_CYCLIC for 3D domain decomposition
# Aligns with GROMACS' PME decomposition
# Minimizes surface-to-volume ratio for communication
partition_scheme=block_cyclic

# Prefetch hint: NEIGHBOR_LIST
# GROMACS generates neighbor lists before force calculation
# Use neighbor list as prefetch directive for particle data
prefetch_mode=neighbor_list

# Consistency model: RELEASE for halo exchange
# Force calculations are embarrassingly parallel within domain
# Synchronization needed at domain boundaries
consistency=release

# Cache line alignment for particle arrays
# Prevents false sharing in force reduction
cache_line_align=true

# Batch size for halo exchange
# Particle positions/forces exchanged between domains
# Typical halo: few thousand particles per boundary
batch_size=256

# Thread configuration
# GROMACS uses hybrid MPI+OpenMP
# 128 OpenMP threads max per domain
num_threads=16

# Domain decomposition hints
# 3D grid decomposition for spatial locality
domain_decomp=3d
dd_order=xyz

# Non-bonded interaction settings
# Verlet neighbor list buffer
neighbor_list_buffer=0.005
nstlist=20

# PME settings for electrostatics
# PME mesh spacing affects memory footprint
pme_order=4
fourier_spacing=0.12

# GPU offload settings (when applicable)
# Coordinate async transfers with CXL operations
gpu_update=true
async_transfer=true

# NUMA optimization
numa_bind=true
