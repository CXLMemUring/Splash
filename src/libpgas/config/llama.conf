# PGAS Configuration for LLAMA LLM Inference
# ===========================================
# LLAMA is a large language model with memory-bandwidth bound inference
# Characteristics:
#   - Decode phase is memory bandwidth bound (low arithmetic intensity <25)
#   - Large model weights (7B=14GB, 70B=140GB, 405B=629GB+ FP16)
#   - Sequential token generation with KV-cache growth
#   - Prefill phase is compute bound, decode is memory bound
#   - Model Bandwidth Utilization (MBU) target: 55-60%
#
# Tuning Strategy:
#   - Maximize memory bandwidth utilization
#   - Interleave model weights across CXL nodes for bandwidth aggregation
#   - Use large batch transfers for weight loading
#   - Optimize for streaming sequential access

# Node configuration
local_node_id=0
num_nodes=2

# Node addresses (IP:Port:CXL_Base:CXL_Size)
# LLAMA-7B needs ~14GB, allocate 16GB per node for weights + KV cache
# For larger models, scale accordingly
node0=127.0.0.1:5000:0x0:17179869184
node1=127.0.0.1:5001:0x0:17179869184

# LLAMA-specific tuning parameters
# =================================

# Memory affinity: INTERLEAVE for bandwidth aggregation
# LLM inference benefits from aggregated bandwidth across CXL nodes
# This enables parallel weight loading from multiple memory pools
memory_affinity=interleave

# Partitioning: BLOCK for layer-wise distribution
# Each transformer layer can be assigned to a node
# Enables pipeline parallelism for multi-node inference
partition_scheme=block

# Prefetch hint: STREAMING
# Model weights are accessed sequentially layer by layer
# KV-cache has predictable growth pattern
prefetch_mode=streaming

# Consistency model: RELAXED
# Inference is read-only for model weights
# KV-cache updates are single-writer
consistency=relaxed

# Large transfer sizes for weight loading
# Transformer layers are typically MBs each
# Amortize CXL latency with large bulk transfers
batch_size=4096
transfer_size=1048576

# Buffer for KV-cache (per sequence, per layer)
# Typical: 2 * num_layers * hidden_dim * max_seq_len * sizeof(float16)
kv_cache_buffer=268435456

# Thread configuration
# Use multiple threads for parallel weight loading
num_threads=8

# Bandwidth optimization hints
# Target >50% MBU for efficient inference
bandwidth_priority=high
numa_interleave=true

# Quantization-aware settings
# INT8 weights are 2x denser, adjust accordingly
weight_dtype=float16
